import logging
from langchain_groq import ChatGroq
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_huggingface.embeddings import HuggingFaceEmbeddings
from config.settings import (
    GROQ_API_KEY, 
    LLM_MODEL_NAME, 
    LLM_TEMPERATURE, 
    LLM_MAX_TOKENS,
    EMBEDDING_MODEL
)

# Initialize logger for the medical chatbot system
logger = logging.getLogger("medical_chatbot")

class LLMService:
    """
    Handles initialization and interaction with both the LLM and embedding models.
    Supports response generation and vector representation of text.
    """

    def __init__(self):
        # Initialize language model and embedding model during service setup
        self.llm = self._initialize_llm()
        self.embeddings = self._initialize_embeddings()

    def _initialize_llm(self):
        """
        Initializes the Groq LLM service using configuration parameters.

        Returns:
            ChatGroq: An instance of the Groq-powered chat model.
        """
        logger.info("Initializing LLM service")
        return ChatGroq(
            api_key=GROQ_API_KEY,
            model_name=LLM_MODEL_NAME,
            temperature=LLM_TEMPERATURE,
            max_tokens=LLM_MAX_TOKENS
        )

    def _initialize_embeddings(self):
        """
        Initializes the embedding model to convert input text into vector representations.

        Returns:
            HuggingFaceEmbeddings: Instance of embedding model (e.g., `all-MiniLM-L6-v2`).
        """
        logger.info("Initializing embeddings model")
        return HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)

    def generate_response(self, prompt: str):
        """
        Sends a prompt to the LLM and returns its generated response.

        Args:
            prompt (str): The input prompt to be sent to the LLM.

        Returns:
            str: Cleaned text output generated by the LLM.
        """
        try:
            logger.debug(f"Generating response for prompt: {prompt[:100]}...")  # Log first 100 characters
            response = self.llm.invoke(prompt)  # Get model response
            return response.content.strip()  # Return stripped content
        except Exception as e:
            logger.error(f"Error generating response: {str(e)}")  # Log and re-raise any exceptions
            raise
